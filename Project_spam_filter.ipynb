{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Spam Filter Project\n",
    "\n",
    "The goal of this project is to construct a spam filter based on the naive Bayes algorithm. To achieve this a dataset of over 5000 SMS messages, that have already been categorized as either spam or ham (not spam) will be used to first train and then test the algorithm.\n",
    "\n",
    "The goal is to reach an accuracy of at least 80%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming the csv file into a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_col = pd.read_csv('SMSSpamCollection', sep = '\\t', header = None, names = ['Label','SMS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if it worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n"
     ]
    }
   ],
   "source": [
    "print(spam_col.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has two columns:\n",
    "* `Label`: either spam or ham\n",
    "* `SMS`: text of the message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the percentage of authentic vs spam messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham     86.593683\n",
      "spam    13.406317\n",
      "Name: Label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(spam_col.Label.value_counts(normalize = True, dropna = False)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large majority of the messages are ham. This is a problem. A model classifying all messages as ham would have an 86.6 % accuracy, but be absolutely useless in classifying the messages. This needs to be taken into consideration both when training and testing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test datasets\n",
    "\n",
    "To ensure a non-biased evaluation and combat overfitting,a separate train and test set will be constructed.\n",
    "\n",
    "**Overfitting** means that the algorithm is very closely shaped around a single dataset. It \"memorizes\" each of the datapoints instead of drawing generalizing. Overfitting is very treacherous, because if an overfitted model is tested on the data it was trained on it will perform very strongly. But when such a model is applied to new data, it will perform very poorly. To combat this it is very helpful to keep the train and test sets completely separate.\n",
    "\n",
    "In this case the dataset will be divided into the two datasets:\n",
    "\n",
    "* `training_set` containing about 80% of all messages\n",
    "* `test_set` set with the remaining 20%.\n",
    "\n",
    "The training set will be used to train our algorithm and the test set will be used to compare the predictions of the algorithm to the manual labeling done by users. We are striving for an accuracy of more than 80%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the two datasets by random sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_spam = spam_col.sample(frac = 1, random_state = 1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4458, 2) (1114, 2)\n",
      "0.8000717875089735\n"
     ]
    }
   ],
   "source": [
    "training_set = random_spam[:4458].copy()\n",
    "test_set = random_spam[4458:].copy()\n",
    "print(training_set.shape, test_set.shape)\n",
    "print(4458/5572)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking to see if the datasets are well randomized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham     86.54105\n",
      "spam    13.45895\n",
      "Name: Label, dtype: float64\n",
      "ham     86.804309\n",
      "spam    13.195691\n",
      "Name: Label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(training_set.Label.value_counts(normalize = True)*100)\n",
    "print(test_set.Label.value_counts(normalize = True)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning:\n",
    "\n",
    "Here several tasks need to be accomplished:\n",
    "* make all words lower case, so capitalization does not impact the count\n",
    "* ignore all punctuation.\n",
    "\n",
    "This will help in the next step, where the dataset will be transformed for easier analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial status of the datasets' heads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Label                                                SMS\n",
      "1078   ham                       Yep, by the pretty sculpture\n",
      "4028   ham      Yes, princess. Are you going to make me moan?\n",
      "958    ham                         Welp apparently he retired\n",
      "4642   ham                                            Havent.\n",
      "4674   ham  I forgot 2 ask 端 all smth.. There's a card on ...\n",
      "     Label                                                SMS\n",
      "2131   ham          Later i guess. I needa do mcat study too.\n",
      "3418   ham             But i haf enuff space got like 4 mb...\n",
      "3424  spam  Had your mobile 10 mths? Update to latest Oran...\n",
      "1538   ham  All sounds good. Fingers . Makes it difficult ...\n",
      "5393   ham  All done, all handed in. Don't know if mega sh...\n"
     ]
    }
   ],
   "source": [
    "print(training_set.head())\n",
    "print(test_set.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each step in data cleaning will be applied to both datasets.\n",
    "\n",
    "In the first step, all characters that are neither letters, digits or underscores (`\\W`) will be replaces with a whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.SMS = training_set.SMS.str.replace('\\W', ' ').str.lower()\n",
    "test_set.SMS = test_set.SMS.str.replace('\\W', ' ').str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Label                                                SMS\n",
      "1078   ham                       yep  by the pretty sculpture\n",
      "4028   ham      yes  princess  are you going to make me moan \n",
      "958    ham                         welp apparently he retired\n",
      "4642   ham                                            havent \n",
      "4674   ham  i forgot 2 ask 端 all smth   there s a card on ...\n",
      "     Label                                                SMS\n",
      "2131   ham          later i guess  i needa do mcat study too \n",
      "3418   ham             but i haf enuff space got like 4 mb   \n",
      "3424  spam  had your mobile 10 mths  update to latest oran...\n",
      "1538   ham  all sounds good  fingers   makes it difficult ...\n",
      "5393   ham  all done  all handed in  don t know if mega sh...\n"
     ]
    }
   ],
   "source": [
    "print(training_set.head())\n",
    "print(test_set.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means all the punctuation has now been removed.\n",
    "\n",
    "Next, each of the messages will be split using the pandas function `.str.split` at the whitespaces into lists and replace the entries in the `SMS` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.SMS = training_set.SMS.str.split()\n",
    "test_set.SMS = test_set.SMS.str.split()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The heads of the datasets now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Label                                                SMS\n",
      "1078   ham                  [yep, by, the, pretty, sculpture]\n",
      "4028   ham  [yes, princess, are, you, going, to, make, me,...\n",
      "958    ham                    [welp, apparently, he, retired]\n",
      "4642   ham                                           [havent]\n",
      "4674   ham  [i, forgot, 2, ask, 端, all, smth, there, s, a,...\n",
      "     Label                                                SMS\n",
      "2131   ham  [later, i, guess, i, needa, do, mcat, study, too]\n",
      "3418   ham      [but, i, haf, enuff, space, got, like, 4, mb]\n",
      "3424  spam  [had, your, mobile, 10, mths, update, to, late...\n",
      "1538   ham  [all, sounds, good, fingers, makes, it, diffic...\n",
      "5393   ham  [all, done, all, handed, in, don, t, know, if,...\n"
     ]
    }
   ],
   "source": [
    "print(training_set.head())\n",
    "print(test_set.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is now formatted to be easier to work with in the next steps!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-formatting the dataset\n",
    "\n",
    "To make training the model easier, the dataset will now be put into a different form. Instead of using one single column with all the words of a given message, each word will now get its own column containing the count of each word in each message.\n",
    "\n",
    "First a list of all the words has to be made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_lst = []\n",
    "for sms in training_set.SMS:\n",
    "    for word in sms:\n",
    "        if word not in vocabulary_lst:\n",
    "            vocabulary_lst.append(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuring that every word is only present once in the new list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    7783\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series(vocabulary_lst).value_counts().value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the indexes in both datasets, since they are still the same as they were before the randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.reset_index(inplace = True)\n",
    "test_set.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=4458, step=1)\n"
     ]
    }
   ],
   "source": [
    "print(training_set.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dictionary `word_count_per_sms` will be created. Each key will be one word from the list of unique words. The value of each key will be a list with the same length as the number of entries in the `training_set`. Each value in the list will be the number the word used as key appears in a single SMS message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_per_sms = {}\n",
    "for word in vocabulary_lst:\n",
    "    word_count_per_sms[word] = [0]*training_set.shape[0]\n",
    "    \n",
    "for index, sms in enumerate(training_set.SMS):\n",
    "    for word in sms:\n",
    "        word_count_per_sms[word][index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = pd.DataFrame(word_count_per_sms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concacenating the labels with the new dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4458, 3) (4458, 7783) (4458, 7784)\n",
      "ham     3858\n",
      "spam     600\n",
      "Name: Label, dtype: int64\n",
      "RangeIndex(start=0, stop=4458, step=1)\n",
      "  Label                                                SMS  yep  by  the  \\\n",
      "0   ham                  [yep, by, the, pretty, sculpture]    1   1    1   \n",
      "1   ham  [yes, princess, are, you, going, to, make, me,...    0   0    0   \n",
      "2   ham                    [welp, apparently, he, retired]    0   0    0   \n",
      "3   ham                                           [havent]    0   0    0   \n",
      "4   ham  [i, forgot, 2, ask, 端, all, smth, there, s, a,...    0   0    0   \n",
      "\n",
      "   pretty  sculpture  yes  princess  are  ...  beauty  hides  secrets  n8  \\\n",
      "0       1          1    0         0    0  ...       0      0        0   0   \n",
      "1       0          0    1         1    1  ...       0      0        0   0   \n",
      "2       0          0    0         0    0  ...       0      0        0   0   \n",
      "3       0          0    0         0    0  ...       0      0        0   0   \n",
      "4       0          0    0         0    0  ...       0      0        0   0   \n",
      "\n",
      "   jewelry  related  trade  arul  bx526  wherre  \n",
      "0        0        0      0     0      0       0  \n",
      "1        0        0      0     0      0       0  \n",
      "2        0        0      0     0      0       0  \n",
      "3        0        0      0     0      0       0  \n",
      "4        0        0      0     0      0       0  \n",
      "\n",
      "[5 rows x 7784 columns]\n"
     ]
    }
   ],
   "source": [
    "# training_set.reset_index(inplace = True)\n",
    "tsdf = pd.concat([training_set, word_count], axis = 1)\n",
    "tsdf = tsdf.drop('index', axis = 1)\n",
    "\n",
    "print(training_set.shape, word_count.shape,tsdf.shape)\n",
    "print(training_set.Label.value_counts(dropna = False))\n",
    "print(training_set.index)\n",
    "print(tsdf.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham     3858\n",
      "spam     600\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(tsdf.Label.value_counts(dropna = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam filter construction\n",
    "\n",
    "The process of cleaning has been completed. Now the spam filter can be constructed.\n",
    "\n",
    "The formula to gauge the probability that something is spam or ham is:\n",
    "\n",
    "$$P(Spam|w_1,w_2,...,w_n) \\propto P(Spam) \\cdot \\prod_{i = 1}^{n} P(w_{i}|Spam)$$\n",
    "\n",
    "$$P(Ham|w_1,w_2,...,w_n) \\propto P(Ham) \\cdot \\prod_{i = 1}^{n} P(w_{i}|Ham)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $P(Spam|w_1,w_2,...,w_n)$ is the probability of spam given the words $w_i$ (Meaning the probability of a message being spam given the word $w_i$ is in the message)\n",
    "* $P(Spam)$ is the probability that any word is spam\n",
    "* $P(w_i|Spam)$ is the probability of $w_i$ given spam\n",
    "\n",
    "\n",
    "And where the Probability for each word given it is spam or ham is:\n",
    "\n",
    "$$P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}} $$\n",
    "\n",
    "$$P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}} $$\n",
    "\n",
    "Here:\n",
    "\n",
    "* $\\alpha$ is the smoothing operator. In the calculation Laplace smoothing will be used. In this case $\\alpha = 1$.\n",
    "* $N_{Vocabulary}$ is the total number of unique words our dataste\n",
    "* $N_{Spam} (or N_{Ham})$ is the total number (not unique) of words in messages categorized as spam (ham)\n",
    "\n",
    "The first step will be to calculate the constants that will be needed in the later calculation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$N_{Spam}$ and $N_{Ham}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_spam = tsdf.loc[tsdf.Label == 'spam'].sum(axis = 1).sum()\n",
    "n_ham = tsdf.loc[tsdf.Label == 'ham'].sum(axis = 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15188\n",
      "57237\n"
     ]
    }
   ],
   "source": [
    "print(n_spam)\n",
    "print(n_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next: $P(Spam)$ & $P(Ham)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13458950201884254\n",
      "0.8654104979811574\n"
     ]
    }
   ],
   "source": [
    "spham_counts = tsdf.Label.value_counts(normalize = True)\n",
    "p_spam = spham_counts['spam']\n",
    "p_ham = spham_counts['ham']\n",
    "print(p_spam)\n",
    "print(p_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$N_{Vocabulary}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7782\n"
     ]
    }
   ],
   "source": [
    "n_vocabulary = tsdf.shape[1] - 2\n",
    "print(n_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point it's a good idea to make another dataframe. It will have one column for spam and one for ham. The indices will be individual words and the values will be the number of times each word appears in each of the categories. This will cut down on computational expense, because when using $N_{W_i}$ the value only needs to be looked up, not counted each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_occur = pd.DataFrame()\n",
    "words_occur['spam'] = tsdf.loc[tsdf.Label == 'spam'].iloc[:,2:].sum(axis = 0)\n",
    "words_occur['ham'] = tsdf.loc[tsdf.Label == 'ham'].iloc[:,2:].sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           spam  ham\n",
      "yep           0    9\n",
      "by           34  110\n",
      "the         157  920\n",
      "pretty        0   12\n",
      "sculpture     0    1\n",
      "...         ...  ...\n",
      "related       0    1\n",
      "trade         0    1\n",
      "arul          0    1\n",
      "bx526         1    0\n",
      "wherre        0    1\n",
      "\n",
      "[7782 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(words_occur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To eliminate one further step in the calculation, instead of using the number of times a word occurs in each group, it will now be replaced by the probabilities of it appearing given spam and ham: $P(w_i|spam)$ and $P(w_i|ham)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "words_probs = words_occur.copy()\n",
    "words_probs.spam = (words_probs.spam + alpha) / (n_spam + (alpha * n_vocabulary))\n",
    "words_probs.ham = (words_probs.ham + alpha)/ (n_ham + (alpha * n_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               spam       ham\n",
      "yep        0.000044  0.000154\n",
      "by         0.001524  0.001707\n",
      "the        0.006879  0.014165\n",
      "pretty     0.000044  0.000200\n",
      "sculpture  0.000044  0.000031\n",
      "...             ...       ...\n",
      "related    0.000044  0.000031\n",
      "trade      0.000044  0.000031\n",
      "arul       0.000044  0.000031\n",
      "bx526      0.000087  0.000015\n",
      "wherre     0.000044  0.000031\n",
      "\n",
      "[7782 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(words_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index Label                                                SMS\n",
      "0   2131   ham  [later, i, guess, i, needa, do, mcat, study, too]\n",
      "1   3418   ham      [but, i, haf, enuff, space, got, like, 4, mb]\n",
      "2   3424  spam  [had, your, mobile, 10, mths, update, to, late...\n",
      "3   1538   ham  [all, sounds, good, fingers, makes, it, diffic...\n",
      "4   5393   ham  [all, done, all, handed, in, don, t, know, if,...\n"
     ]
    }
   ],
   "source": [
    "print(test_set.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictulator(row):\n",
    "    '''Predict if a message is either ham or spam\n",
    "    Args:\n",
    "        row (int): row of the message to predict\n",
    "    Returns:\n",
    "        str: predicted category of the message\n",
    "    '''\n",
    "    spam_prob = p_spam\n",
    "    ham_prob = p_ham\n",
    "    for item in row:\n",
    "        if item in words_probs.index:\n",
    "            spam_prob *= words_probs.loc[item, 'spam']\n",
    "            ham_prob *= words_probs.loc[item,'ham']\n",
    "        else:\n",
    "            spam_prob *= (alpha / (alpha * n_vocabulary))\n",
    "            ham_prob *= (alpha / (alpha * n_vocabulary))\n",
    "    if spam_prob > ham_prob:\n",
    "        return 'spam'\n",
    "    elif ham_prob > spam_prob:\n",
    "        return 'ham'\n",
    "    else:\n",
    "        return'equal'\n",
    "\n",
    "test_set['predict'] = test_set.SMS.apply(predictulator)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True     98.743268\n",
      "False     1.256732\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print((test_set.Label == test_set.predict).value_counts(normalize = True)* 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "When using the accuracy metric, the spam filter was 98.7% accurate. This is a very good result. Unfortunately this metric does not take into account, that the large majority of messages are of the category ham. This could be looked at in more detail in further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this project, a spam filter was constructed for use on SMS messages, using a dataset of about 5000 messages. The spam filter was constructed using the naive Bayes algorithm. This algorithm is very good at categorizing messages. In total the spam filter scored 98.7% accuracy when categorizing the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
