{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Spam Filter Project\n",
    "\n",
    "The goal of this project is to construct a spam filter based on the naive bayes algorithm. To achieve this we will use a dataset of over 5000 SMS messages that were already categorized as either spam or non-spam to teach the algorithm. We will then apply the resulting algorithm to more messages. Our spam filter schould have an accuracy of at least 80%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming the csv file into a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_col = pd.read_csv('SMSSpamCollection', sep = '\\t', header = None, names = ['Label','SMS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if it worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n"
     ]
    }
   ],
   "source": [
    "print(spam_col.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dataset has two columns, corresponding to the assigned label and the text in the SMS message.\n",
    "We can also say that the messages were either labeled 'spam' or 'ham'. The second one being for authentic messages, this makes sense as ham is known to be way more authentic than spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the percentage of authentic vs spam messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham     86.593683\n",
      "spam    13.406317\n",
      "Name: Label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(spam_col.Label.value_counts(normalize = True, dropna = False)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have approximately 7 times as many authentic messages as we do spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Spam Filter:\n",
    "\n",
    "To ensure, that we are not biased and design a test for the spam filter after we have made it, we will do that now. To test how good our spam filter ist, we will divide the dataset into two different datasets:\n",
    "* a training set containing about 80% of all messaget\n",
    "* a test set with the remaining 20%.\n",
    "\n",
    "The training set will be used to train our algorithm and the test set will be used to compare the predictions of the algorithm to the manual labeling done by users. We are striving for an accuracy of more than 80%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the two datasets by random sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_spam = spam_col.sample(frac = 1, random_state = 1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4458, 2) (1114, 2)\n",
      "0.8000717875089735\n"
     ]
    }
   ],
   "source": [
    "training_set = random_spam[:4458].copy()\n",
    "test_set = random_spam[4458:].copy()\n",
    "print(training_set.shape, test_set.shape)\n",
    "print(4458/5572)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking to see if the datasets are well randomized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham     86.54105\n",
      "spam    13.45895\n",
      "Name: Label, dtype: float64\n",
      "ham     86.804309\n",
      "spam    13.195691\n",
      "Name: Label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(training_set.Label.value_counts(normalize = True)*100)\n",
    "print(test_set.Label.value_counts(normalize = True)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning:\n",
    "\n",
    "Here we want to accomplish several tasks. We want to:\n",
    "* make all words lower case, so capitalization does not impact the count\n",
    "* ignore all punctuation.\n",
    "\n",
    "We do this, because in the following step we want to transform our dataset for easier analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial status of the datasets' heads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Label                                                SMS\n",
      "1078   ham                       Yep, by the pretty sculpture\n",
      "4028   ham      Yes, princess. Are you going to make me moan?\n",
      "958    ham                         Welp apparently he retired\n",
      "4642   ham                                            Havent.\n",
      "4674   ham  I forgot 2 ask Ã¼ all smth.. There's a card on ...\n",
      "     Label                                                SMS\n",
      "2131   ham          Later i guess. I needa do mcat study too.\n",
      "3418   ham             But i haf enuff space got like 4 mb...\n",
      "3424  spam  Had your mobile 10 mths? Update to latest Oran...\n",
      "1538   ham  All sounds good. Fingers . Makes it difficult ...\n",
      "5393   ham  All done, all handed in. Don't know if mega sh...\n"
     ]
    }
   ],
   "source": [
    "print(training_set.head())\n",
    "print(test_set.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be doing the same data cleaning for both the training and test dataset, because it's the same step and we won't forget in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.SMS = training_set.SMS.str.replace('\\W', ' ').str.lower()\n",
    "test_set.SMS = test_set.SMS.str.replace('\\W', ' ').str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.SMS = training_set.SMS.str.split()\n",
    "test_set.SMS = test_set.SMS.str.split()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The heads of the datasets now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Label                                                SMS\n",
      "1078   ham                  [yep, by, the, pretty, sculpture]\n",
      "4028   ham  [yes, princess, are, you, going, to, make, me,...\n",
      "958    ham                    [welp, apparently, he, retired]\n",
      "4642   ham                                           [havent]\n",
      "4674   ham  [i, forgot, 2, ask, Ã¼, all, smth, there, s, a,...\n",
      "     Label                                                SMS\n",
      "2131   ham          later i guess  i needa do mcat study too \n",
      "3418   ham             but i haf enuff space got like 4 mb   \n",
      "3424  spam  had your mobile 10 mths  update to latest oran...\n",
      "1538   ham  all sounds good  fingers   makes it difficult ...\n",
      "5393   ham  all done  all handed in  don t know if mega sh...\n"
     ]
    }
   ],
   "source": [
    "print(training_set.head())\n",
    "print(test_set.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we can clearly see by the message 'yes  princes  are you going to make me moan' it worked!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-formatting the dataset\n",
    "\n",
    "\n",
    "We will now change the format of the dataset to replace the SMS column with columns for each individual word. We will begin, by making a list of all the unique words in our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_lst = []\n",
    "for sms in training_set.SMS:\n",
    "    for word in sms:\n",
    "        if word not in vocabulary_lst:\n",
    "            vocabulary_lst.append(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finding out if every word is only present once in the new list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    7783\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series(vocabulary_lst).value_counts().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.reset_index(inplace = True)\n",
    "test_set.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=4458, step=1)\n"
     ]
    }
   ],
   "source": [
    "print(training_set.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instructions want us to change the list into a set and then change it back into a list, to disable any duplicates, but since we already did this and the length of 'vocabulary_lst' matches up with the list in the solution to this project, we are good to skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a dictionary 'word_count_per_sms'. This will contain all the words in our vocabulary as keys. The values will be a list with the length of the dataset. The indices will be the sms messages in our dataset. The value of the each index will be the ammount of times the word in the key appears in a given message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_per_sms = {}\n",
    "for word in vocabulary_lst:\n",
    "    word_count_per_sms[word] = [0]*training_set.shape[0]\n",
    "    \n",
    "for index, sms in enumerate(training_set.SMS):\n",
    "    for word in sms:\n",
    "        word_count_per_sms[word][index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = pd.DataFrame(word_count_per_sms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concacenating the labels with our new dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4458, 3) (4458, 7783) (4458, 7784)\n",
      "ham     3858\n",
      "spam     600\n",
      "Name: Label, dtype: int64\n",
      "RangeIndex(start=0, stop=4458, step=1)\n",
      "  Label                                                SMS  yep  by  the  \\\n",
      "0   ham                  [yep, by, the, pretty, sculpture]    1   1    1   \n",
      "1   ham  [yes, princess, are, you, going, to, make, me,...    0   0    0   \n",
      "2   ham                    [welp, apparently, he, retired]    0   0    0   \n",
      "3   ham                                           [havent]    0   0    0   \n",
      "4   ham  [i, forgot, 2, ask, Ã¼, all, smth, there, s, a,...    0   0    0   \n",
      "\n",
      "   pretty  sculpture  yes  princess  are  ...  beauty  hides  secrets  n8  \\\n",
      "0       1          1    0         0    0  ...       0      0        0   0   \n",
      "1       0          0    1         1    1  ...       0      0        0   0   \n",
      "2       0          0    0         0    0  ...       0      0        0   0   \n",
      "3       0          0    0         0    0  ...       0      0        0   0   \n",
      "4       0          0    0         0    0  ...       0      0        0   0   \n",
      "\n",
      "   jewelry  related  trade  arul  bx526  wherre  \n",
      "0        0        0      0     0      0       0  \n",
      "1        0        0      0     0      0       0  \n",
      "2        0        0      0     0      0       0  \n",
      "3        0        0      0     0      0       0  \n",
      "4        0        0      0     0      0       0  \n",
      "\n",
      "[5 rows x 7784 columns]\n"
     ]
    }
   ],
   "source": [
    "# training_set.reset_index(inplace = True)\n",
    "tsdf = pd.concat([training_set, word_count], axis = 1)\n",
    "tsdf = tsdf.drop('index', axis = 1)\n",
    "\n",
    "print(training_set.shape, word_count.shape,tsdf.shape)\n",
    "print(training_set.Label.value_counts(dropna = False))\n",
    "print(training_set.index)\n",
    "print(tsdf.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham     3858\n",
      "spam     600\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(tsdf.Label.value_counts(dropna = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam filter construction\n",
    "\n",
    "We are now done with the process of cleaning the data and will move on to constructing the actual spam filter.\n",
    "The formula to gauge the probability that somethin is spam or ham is:\n",
    "\n",
    "$$P(Spam|w_1,w_2,...,w_n) \\propto P(Spam) \\cdot \\prod_{i = 1}^{n} P(w_{i}|Spam)$$\n",
    "\n",
    "$$P(Ham|w_1,w_2,...,w_n) \\propto P(Ham) \\cdot \\prod_{i = 1}^{n} P(w_{i}|Ham)$$\n",
    "\n",
    "Where the Probability for each word given it is spam or ham is:\n",
    "\n",
    "$$P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}} $$\n",
    "\n",
    "$$P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}} $$\n",
    "\n",
    "Here:\n",
    "\n",
    "* $\\alpha$ is the smoothing operator. We will be using Laplace smoothing where $\\alpha = 1$\n",
    "* $N_{Vocabulary}$ is the total number of unique words our dataste\n",
    "* $N_{Spam} (or N_{Ham})$ is the total number (not unique) of words in messages categorized as spam (ham)\n",
    "\n",
    "We will start by calculating the constants we will be using in the calculation later:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Terrible solution:\n",
    "n_spam = 0\n",
    "n_ham = 0\n",
    "indices = np.arange(tsdf.shape[0])\n",
    "for i in indices:\n",
    "    row = tsdf.iloc[i]\n",
    "    if row.iloc[0] == 'spam':\n",
    "        n_spam += row.iloc[2:].sum()\n",
    "    elif row.iloc[0] == 'ham':\n",
    "        n_ham += row.iloc[2:].sum()\n",
    "    else:\n",
    "        print(row.iloc[0],i)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(n_spam)\n",
    "print(n_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same results but much faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_spam = tsdf.loc[tsdf.Label == 'spam'].sum(axis = 1).sum()\n",
    "n_ham = tsdf.loc[tsdf.Label == 'ham'].sum(axis = 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15188\n",
      "57237\n"
     ]
    }
   ],
   "source": [
    "print(n_spam)\n",
    "print(n_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will calculate $P(Spam)$ & $P(Ham)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13458950201884254\n",
      "0.8654104979811574\n"
     ]
    }
   ],
   "source": [
    "spham_counts = tsdf.Label.value_counts(normalize = True)\n",
    "p_spam = spham_counts['spam']\n",
    "p_ham = spham_counts['ham']\n",
    "print(p_spam)\n",
    "print(p_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7782\n"
     ]
    }
   ],
   "source": [
    "n_vocabulary = tsdf.shape[1] - 2\n",
    "print(n_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also make two more dataframes, that have the words as keys and the number of times they appear in either spam or ham messages as the values. This will cut down on computational expense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_occur = pd.DataFrame()\n",
    "words_occur['spam'] = tsdf.loc[tsdf.Label == 'spam'].iloc[:,2:].sum(axis = 0)\n",
    "words_occur['ham'] = tsdf.loc[tsdf.Label == 'ham'].iloc[:,2:].sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           spam  ham\n",
      "yep           0    9\n",
      "by           34  110\n",
      "the         157  920\n",
      "pretty        0   12\n",
      "sculpture     0    1\n",
      "...         ...  ...\n",
      "related       0    1\n",
      "trade         0    1\n",
      "arul          0    1\n",
      "bx526         1    0\n",
      "wherre        0    1\n",
      "\n",
      "[7782 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(words_occur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let' turn this dataframe from occurences to the probabilities $P(w_i|spam)$ and $P(w_i|ham)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_probs = words_occur.copy()\n",
    "words_probs.spam = (words_probs.spam + alpha) / (n_spam + (alpha * n_vocabulary))\n",
    "words_probs.ham = (words_probs.ham + alpha)/ (n_ham + (alpha * n_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               spam       ham\n",
      "yep        0.000044  0.000154\n",
      "by         0.001524  0.001707\n",
      "the        0.006879  0.014165\n",
      "pretty     0.000044  0.000200\n",
      "sculpture  0.000044  0.000031\n",
      "...             ...       ...\n",
      "related    0.000044  0.000031\n",
      "trade      0.000044  0.000031\n",
      "arul       0.000044  0.000031\n",
      "bx526      0.000087  0.000015\n",
      "wherre     0.000044  0.000031\n",
      "\n",
      "[7782 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(words_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1 #which is apparently necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index Label                                                SMS\n",
      "0   2131   ham  [later, i, guess, i, needa, do, mcat, study, too]\n",
      "1   3418   ham      [but, i, haf, enuff, space, got, like, 4, mb]\n",
      "2   3424  spam  [had, your, mobile, 10, mths, update, to, late...\n",
      "3   1538   ham  [all, sounds, good, fingers, makes, it, diffic...\n",
      "4   5393   ham  [all, done, all, handed, in, don, t, know, if,...\n"
     ]
    }
   ],
   "source": [
    "print(test_set.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def throbulator(row):\n",
    "    spam_prob = p_spam\n",
    "    ham_prob = p_ham\n",
    "    for item in row:\n",
    "        if item in words_probs.index:\n",
    "            spam_prob *= words_probs.loc[item, 'spam']\n",
    "            ham_prob *= words_probs.loc[item,'ham']\n",
    "        else:\n",
    "            spam_prob *= (alpha / (alpha * n_vocabulary))\n",
    "            ham_prob *= (alpha / (alpha * n_vocabulary))\n",
    "    if spam_prob > ham_prob:\n",
    "        return 'spam'\n",
    "    elif ham_prob > spam_prob:\n",
    "        return 'ham'\n",
    "    else:\n",
    "        return'equal'\n",
    "\n",
    "test_set['predict'] = test_set.SMS.apply(throbulator)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True     98.743268\n",
      "False     1.256732\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print((test_set.Label == test_set.predict).value_counts(normalize = True)* 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means our spam filter was 98.7% accurate. This worked amazingly!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
